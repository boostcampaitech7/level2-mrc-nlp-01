model:
  name: klue/bert-base
  retriever_tokenizer: Dongjin-kr/ko-reranker

seed: 2024

training:
  epochs: 3 # HF default = 3
  batch_size: 16 # HF default = 8
  learning_rate: 5e-5 # HF default = 5e-5
  optimizer: AdamW # HF default = AdamW
  loss: L1loss # Not Implementedd
  shuffle: True # Not Implemented
  weight_decay: 0 # HF default = 0.0
  scheduler: linear # HF default = "linear"
  predict_with_generate: False # If you want to generate, set True
  logging_steps: 10
  adam_eps: 1e-8

peft:
  LoRA: False # If you want to apply LoRA, set True
  task_type: SEQ_2_SEQ_LM
  inference: False # default = False
  r: 8 # default = 8
  lora_alpha: 32 # default = 32
  lora_dropout: 0.1 # default = 0.1

dataRetrieval:
  type: hybrid1
  eval: True
  context_path: /data/ephemeral/home/ksw/level2-mrc-nlp-01/data/wikipedia_documents.json
  top_k: 10
  hybrid_ratio: 0.5
  faiss:
    use: False
    num_clusters: 64

dataQA:
  useDataset: KorQuAD/squad_kor_v1
  path: 
    train: ./data/train_dataset
    test: ./data/test_dataset
  tokenizer:
    max_seq_length: 384
    max_answer_length: 64
    doc_stride: 128
    pad_to_max_length: True
    preprocessing_num_workers: null
    overwrite_cache: False
  generation:
    num_beams: 3

  preprocess:
    # - method: your-preprocess-method
    #   params:
    #     p: 0.0

  augmentation:
    # - method: your-augementation-method
    #   params:
    #     p: 0.0

output:
  model: ./models/train_dataset
  train: ./outputs/train_dataset
  test: ./outputs/test_dataset

testing: False

dataset:
  train_path: ./data/train_dataset
