model:
  name: timpal0l/mdeberta-v3-base-squad2
  retriever_tokenizer: Dongjin-kr/ko-reranker

seed: 2024

training:
  epochs: 3 # HF default = 3
  batch_size: 32 # HF default = 8
  learning_rate: 2e-5 # HF default = 5e-5
  # optimizer: AdamW # HF default = AdamW 반영 안 되고 있으나 기본적으로 adamW이기에 상관 X
  # loss: L1loss # Not Implementedd
  # shuffle: True # Not Implemented # Trainer는 DataLoader 생성시 자동으로 shuffle해준다.
  weight_decay: 0.01 # HF default = 0.0
  scheduler: cosine # HF default = "linear"
  predict_with_generate: False # If you want to generate, set True
  logging_steps: 10
  adam_eps: 1e-8
  max_grad_norm: 2.0

peft:
  LoRA: False # If you want to apply LoRA, set True
  task_type: SEQ_2_SEQ_LM
  inference: False # default = False
  r: 8 # default = 8
  lora_alpha: 32 # default = 32
  lora_dropout: 0.1 # default = 0.1

dataRetrieval:
  type: sparse   # sparse dense hybrid
  eval: True
  context_path: ./data/wikipedia_documents.json
  top_k: 10
  faiss:
    use: False
    num_clusters: 64

dataQA:
  useDataset: KorQuAD/squad_kor_v1
  path: 
    train: ./data/train_dataset
    test: ./data/test_dataset
  tokenizer:
    max_seq_length: 384
    max_answer_length: 64
    doc_stride: 128
    pad_to_max_length: True
    preprocessing_num_workers: 6
    overwrite_cache: False
  generation:
    num_beams: 3

  preprocess:
    # - method: your-preprocess-method
    #   params:
    #     p: 0.0

  augmentation:
    # - method: your-augementation-method
    #   params:
    #     p: 0.0

output:
  model: ./models/train_dataset
  train: ./outputs/train_dataset
  test: ./outputs/test_dataset

testing: False
