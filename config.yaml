model:
  name: monologg/koelectra-base-v3-finetuned-korquad

seed: 2024

training:
  epochs: 3 # HF default = 3
  batch_size: 32 # HF default = 8
  learning_rate: 5e-5 # HF default = 5e-5
  optimizer: AdamW # HF default = AdamW
  loss: L1loss # Not Implementedd
  shuffle: True # Not Implemented
  weight_decay: 0 # HF default = 0.0
  scheduler: linear # HF default = "linear"
  predict_with_generate: False # If you want to generate, set True

dataRetrieval:
  type: hybrid1  # "sparse", "dense", 또는 "hybrid1"
  eval: True
  data_path: "/data/ephemeral/home/ksw/level2-mrc-nlp-01/data"
  context_path: ./data/wikipedia_documents.json
  top_k: 10
  hybrid_ratio: 0.7
  faiss:
    use: False
    num_clusters: 64

dataQA:
  path: 
    train: ./data/train_dataset
    test: ./data/test_dataset
  tokenizer:
    max_seq_length: 384
    max_answer_length: 30
    doc_stride: 128
    pad_to_max_length: True
    preprocessing_num_workers: null
    overwrite_cache: False
  generation:
    num_beams: 3

  preprocess:
    # - method: your-preprocess-method
    #   params:
    #     p: 0.0

  augmentation:
    # - method: your-augementation-method
    #   params:
    #     p: 0.0

output:
  model: ./models/train_dataset
  train: ./outputs/train_dataset
  test: ./outputs/test_dataset

testing: False
