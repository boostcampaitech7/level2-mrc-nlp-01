output_dir ./outputs/train_dataset
Debug: config.dataRetrieval = {'type': 'hybrid1', 'eval': True, 'context_path': '/data/ephemeral/home/ksw/level2-mrc-nlp-01/data/wikipedia_documents.json', 'top_k': 10, 'hybrid_ratio': 0.5, 'faiss': {'use': False, 'num_clusters': 64}}
Debug: config.dataRetrieval.context_path = /data/ephemeral/home/ksw/level2-mrc-nlp-01/data/wikipedia_documents.json
Debug: config.dataRetrieval = {'type': 'hybrid1', 'eval': True, 'context_path': '/data/ephemeral/home/ksw/level2-mrc-nlp-01/data/wikipedia_documents.json', 'top_k': 10, 'hybrid_ratio': 0.5, 'faiss': {'use': False, 'num_clusters': 64}}
Debug: config.dataRetrieval.context_path = /data/ephemeral/home/ksw/level2-mrc-nlp-01/data/wikipedia_documents.json
Debug: context_path = /data/ephemeral/home/ksw/level2-mrc-nlp-01/data/wikipedia_documents.json
Lengths of unique contexts : 56737
Lengths of unique contexts : 56737
Debug: dense_retriever.context_path = /data/ephemeral/home/ksw/level2-mrc-nlp-01/data/wikipedia_documents.json
Debug: dense_retriever.model_name = klue/bert-base
Debug: Initializing Hybrid1StageRetrieval
Initializing hybrid_1stage with ratio: 0.5
Initializing hybrid_1stage with top_k: 10
Debug: config.dataRetrieval = {'type': 'hybrid1', 'eval': True, 'context_path': '/data/ephemeral/home/ksw/level2-mrc-nlp-01/data/wikipedia_documents.json', 'top_k': 10, 'hybrid_ratio': 0.5, 'faiss': {'use': False, 'num_clusters': 64}}
Debug: config.dataRetrieval.hybrid_ratio = 0.5
Generating dense embeddings...
Build passage embedding
Training encoder
Lengths of unique contexts : 56737
-----------BM25 pickle loaded.-----------
Prepare in-batch negatives:   0%|                                                               | 0/3952 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/data/ephemeral/home/ksw/level2-mrc-nlp-01/src/main.py", line 579, in <module>
    main()
  File "/data/ephemeral/home/ksw/level2-mrc-nlp-01/src/main.py", line 576, in main
    do_retrieval(config, training_args, logger, is_testing)
  File "/data/ephemeral/home/ksw/level2-mrc-nlp-01/src/main.py", line 402, in do_retrieval
    retriever = Hybrid1StageRetrieval(
  File "/data/ephemeral/home/ksw/level2-mrc-nlp-01/src/Retrieval/hybrid_1stage.py", line 45, in __init__
    self.dense_retriever.get_dense_embedding()
  File "/data/ephemeral/home/ksw/level2-mrc-nlp-01/src/Retrieval/dense_retrieval.py", line 320, in get_dense_embedding
    self.train()
  File "/data/ephemeral/home/ksw/level2-mrc-nlp-01/src/Retrieval/dense_retrieval.py", line 201, in train
    self.prepare_in_batch_negative()
  File "/data/ephemeral/home/ksw/level2-mrc-nlp-01/src/Retrieval/dense_retrieval.py", line 166, in prepare_in_batch_negative
    neg_samples = [self.contexts[neg_idx[j]] for j in range(num_neg+1) if self.contexts[neg_idx[j]] != dataset["context"][i]]
TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'
