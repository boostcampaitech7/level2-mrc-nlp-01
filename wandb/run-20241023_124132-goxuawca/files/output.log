output_dir ./models/train_dataset
Lengths of unique contexts : 56737
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Lengths of unique contexts : 56737
Build BM25 embedding
Token indices sequence length is longer than the specified maximum sequence length for this model (1131 > 512). Running this sequence through the model will result in indexing errors
Parallel tokenizing: 100%|█████████████████████████████████████████████████████████| 56737/56737 [00:01<00:00, 51828.99it/s]
-----------BM25 pickle saved.-----------
Training encoder
get_relevant_doc_bulk 실행중 queries의 개수는  3952
get_relevant_doc_bulk 실행중 queries의 type은  <class 'list'>
self.process_single_query하는중
self.process_single_query끝남
Processing queries: 100%|███████████████████████████████████████████████████████████████| 3952/3952 [41:39<00:00,  1.58it/s]
hi
Filter: 100%|█████████████████████████████████████████████████████████████████| 3952/3952 [00:00<00:00, 20331.03 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████| 3952/3952 [00:00<00:00, 8460.12 examples/s]
Prepare in-batch negatives: 100%|███████████████████████████████████████████████████████| 3952/3952 [04:59<00:00, 13.20it/s]
/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
100%|████████████████████| 1976/1976 [36:51<00:00,  1.12s/batch, loss=1.8179249309469014e-05, lr=0.00001334, step=1976/5928]
100%|█████████████████████| 1976/1976 [36:35<00:00,  1.11s/batch, loss=3.278249096183572e-06, lr=0.00000667, step=3952/5928]
100%|█████████████████████| 1976/1976 [30:09<00:00,  1.09batch/s, loss=4.410727797221625e-06, lr=0.00000000, step=5928/5928]
Epoch: 100%|██████████████████████████████████████████████████████████████████████████████| 3/3 [1:43:36<00:00, 2072.26s/it]
